{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62a82c45",
   "metadata": {},
   "source": [
    "#Explanation:\n",
    "Deep Learning is a subset of Machine Learning that focuses on models with multiple layers, known as neural networks. These models are designed to simulate the human brain's neural structure, allowing them to learn from vast amounts of data. The key concepts in Deep Learning include:\n",
    "\n",
    "Neurons and Layers: The basic unit of a neural network is a neuron, which receives input, processes it, and produces output. Neurons are organized into layers: input layer, hidden layers, and output layer.\n",
    "\n",
    "Activation Functions: These functions introduce non-linearity into the model, allowing it to learn complex patterns. Common activation functions include ReLU (Rectified Linear Unit), sigmoid, and tanh.\n",
    "\n",
    "Forward Propagation: This is the process of passing input data through the network to generate an output.\n",
    "\n",
    "Loss Function: This measures the difference between the predicted output and the actual output. The goal is to minimize this loss.\n",
    "\n",
    "Backpropagation: A method used to update the weights of the network by minimizing the loss function through gradient descent.\n",
    "\n",
    "Learning Rate: A hyperparameter that controls how much to change the model in response to the estimated error each time the model weights are updated.\n",
    "\n",
    "Epochs and Batches: Epochs refer to one complete pass through the entire training dataset, while batches are subsets of the dataset used to train the model in parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "546c7092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.2550\n",
      "Epoch 100, Loss: 0.2500\n",
      "Epoch 200, Loss: 0.2500\n",
      "Epoch 300, Loss: 0.2500\n",
      "Epoch 400, Loss: 0.2500\n",
      "Epoch 500, Loss: 0.2500\n",
      "Epoch 600, Loss: 0.2500\n",
      "Epoch 700, Loss: 0.2500\n",
      "Epoch 800, Loss: 0.2500\n",
      "Epoch 900, Loss: 0.2500\n",
      "Predicted Output:\n",
      " [[0.5       ]\n",
      " [0.49999679]\n",
      " [0.50000434]\n",
      " [0.50000113]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Activation functions\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Derivatives of activation functions\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "# Loss function (Mean Squared Error)\n",
    "def mse_loss(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "# Forward and backward propagation in a simple neural network\n",
    "class SimpleNeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # Weights initialization\n",
    "        self.weights_input_hidden = np.random.rand(input_size, hidden_size)\n",
    "        self.weights_hidden_output = np.random.rand(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Forward pass\n",
    "        self.hidden = relu(np.dot(X, self.weights_input_hidden))\n",
    "        output = sigmoid(np.dot(self.hidden, self.weights_hidden_output))\n",
    "        return output\n",
    "\n",
    "    def backward(self, X, y_true, y_pred, learning_rate):\n",
    "        # Backward pass\n",
    "        output_error = y_pred - y_true\n",
    "        hidden_error = np.dot(output_error, self.weights_hidden_output.T) * relu_derivative(self.hidden)\n",
    "\n",
    "        # Update weights\n",
    "        self.weights_hidden_output -= learning_rate * np.dot(self.hidden.T, output_error)\n",
    "        self.weights_input_hidden -= learning_rate * np.dot(X.T, hidden_error)\n",
    "\n",
    "    def train(self, X, y_true, epochs, learning_rate):\n",
    "        for epoch in range(epochs):\n",
    "            y_pred = self.forward(X)\n",
    "            loss = mse_loss(y_true, y_pred)\n",
    "            self.backward(X, y_true, y_pred, learning_rate)\n",
    "            if epoch % 100 == 0:\n",
    "                print(f'Epoch {epoch}, Loss: {loss:.4f}')\n",
    "\n",
    "# Example usage\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])  # XOR problem\n",
    "\n",
    "nn = SimpleNeuralNetwork(input_size=2, hidden_size=2, output_size=1)\n",
    "nn.train(X, y, epochs=1000, learning_rate=0.1)\n",
    "\n",
    "# Prediction\n",
    "output = nn.forward(X)\n",
    "print(\"Predicted Output:\\n\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debb2dd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
